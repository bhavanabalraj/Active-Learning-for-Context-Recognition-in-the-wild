# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HYZA3ncR7nYxIabBmJFvhjuFYxv72OyO
"""

import gzip
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
from torch import optim
from os import listdir
import math
from numpy.random import choice
import copy
import pickle
import sys

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

def parse_header_of_csv(user_context_data):

    columns = list(user_context_data.columns)

    # The first column should be timestamp:
    assert columns[0] == 'timestamp';
    # The last column should be label_source:
    assert columns[-1] == 'label_source';

    # Search for the column of the first label:
    for (ci,col) in enumerate(columns):
        if col.startswith('label:'):
            first_label_ind = ci;
            break;
        pass;

    # Feature columns come after timestamp and before the labels:
    feature_names = columns[1:first_label_ind]
    # Then come the labels, till the one-before-last column:
    label_names = columns[first_label_ind:-1]

    for (li,label) in enumerate(label_names):
        # In the CSV the label names appear with prefix 'label:', but we don't need it after reading the data:
        assert label.startswith('label:')
        label_names[li] = label.replace('label:','')
        pass

    return (feature_names,label_names);

def parse_body_of_csv(user_context_data,n_features):

    # Read the entire CSV body into a single numeric matrix:
    # full_table = np.loadtxt(StringIO.StringIO(csv_str),delimiter=',',skiprows=1);

    # Timestamp is the primary key for the records (examples):
    timestamps = user_context_data['timestamp']

    # Read the sensor features:
    X = user_context_data.iloc[:,1:(n_features+1)]

    # Read the binary label values, and the 'missing label' indicators:
    # trinary_labels_mat = user_context_data.iloc[:, (n_features+1):-1]; # This should have values of either 0., 1. or NaN
    # M = np.isnan(trinary_labels_mat); # M is the missing label matrix
    # Y = np.where(M,0,trinary_labels_mat) > 0.; # Y is the label matrix

    Y = user_context_data.iloc[:,(n_features+1):-1]; # This should have values of either 0., 1. or NaN
    M = np.isnan(Y)

    return (X,Y,M,timestamps)

def read_user_data(source_dir, uuid):

    user_data_file = source_dir + '%s.features_labels.csv.gz' % uuid

    user_context_data = \
            pd.read_csv(user_data_file, compression='gzip', header=0, sep=',', \
                quotechar='"', error_bad_lines=False)

    (feature_names,label_names) = parse_header_of_csv(user_context_data)
    n_features = len(feature_names)
    (X,Y,M,timestamps) = parse_body_of_csv(user_context_data,n_features)

    return (X,Y,M,timestamps,feature_names,label_names)

def read_data_from_all_users(source_dir='../ESDataset/'):

    users = listdir(source_dir)
    users = [user.split('.', 1)[0] for user in users]

    print("There are ", len(users), " users")

    X = pd.DataFrame()
    Y = pd.DataFrame()
    timestamps = pd.DataFrame()
    M = pd.DataFrame()

    for i in range(len(users)):
        uuid = users[i]
        (X_user, Y_user, M_user, timestamp_user, feature_names, label_names) =\
                                                read_user_data(source_dir, uuid)
        X = X.append(X_user)
        Y = Y.append(Y_user)
        timestamps = timestamps.append(timestamp_user)
        M = M.append(M_user)

        if i % 5 == 0:
            print("Read %d users data" %i)

    return users, X, Y, M, timestamps, label_names

def get_class_weights(y_train, M):

   y_train_present = y_train * np.logical_not(M)

   pos_weights = torch.sum(y_train_present, dim=0)
   missing_counts = torch.sum(M, dim=0)

   y_train_neg = np.logical_and(np.logical_not(y_train_present), np.logical_not(M))
   neg_weights = torch.sum(y_train_neg, dim=0)

   pos_weights = (len(y_train) - missing_counts) / pos_weights
   neg_weights = (len(y_train) - missing_counts) / neg_weights

   pos_weights = y_train_present * pos_weights
   neg_weights = y_train_neg * neg_weights

   pos_weights[np.isnan(pos_weights)] = 0.
   neg_weights[np.isnan(neg_weights)] = 0.

   pos_weights[pos_weights == float('inf')] = 0.
   neg_weights[neg_weights == float('inf')] = 0.

   instance_weights = (pos_weights + neg_weights)

   return instance_weights

def myLoss(model, predicts, y_output, instance_weights):

    criterion = nn.BCEWithLogitsLoss(reduction='none')
    loss = criterion(predicts, y_output)
    loss = loss * instance_weights

    l1_reg = torch.tensor(0., requires_grad=True)
    l1_lambda = 5e-4
    for name, param in model.named_parameters():
        if 'weight' in name:
            l1_reg = l1_reg + torch.norm(param, 1)

    loss = torch.mean(loss) + (l1_lambda * l1_reg)
    return loss

# Classifier
class Network(nn.Module):

    def __init__(self):

        super(Network, self).__init__()
        # Inputs to hidden layer linear transformation
        self.h1 = nn.Linear(176, 16)
        self.activation = nn.LeakyReLU()
        self.h2 = nn.Linear(16, 16)
        self.output = nn.Linear(16, 51)

    def forward(self, x):

        # Hidden layer with Leaky ReLU activation
        x = self.activation(self.h1(x))
        x = self.activation(self.h2(x))

        # Output layer
        x = self.output(x)
        return x

criterion = nn.BCEWithLogitsLoss()
train_losses, test_losses = [], []
accuracies = []
epochs = 40
batch_size = 300

#Train model
def trainAndTestModel(model, X_train, y_train, X_test, y_test, train_weights, M_test):

    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)
    lre = 0.01
    trainLoaderSize = len(X_train)/batch_size
    max_ba = 0
    max_sen = 0
    max_spec = 0
    max_pre = 0
    max_f_score = 0
    max_test_loss = 0
    max_mba = 0
    max_emr = 0
    confusion_dict = 0

    for e in range(epochs):

        running_loss = 0
        permutation = torch.randperm(X_train.size()[0])

        for param_group in optimizer.param_groups:
            param_group['lr'] = lre

        for i in range(0, X_train.size()[0], batch_size):

            optimizer.zero_grad()

            indices = permutation[i:i+batch_size]
            batch_x, batch_y = X_train[indices], y_train[indices]
            batch_weights = train_weights[indices]

            log_ps = model(batch_x)
            loss = myLoss(model, log_ps, batch_y, batch_weights)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        else:

            train_losses.append(running_loss/trainLoaderSize)
            print("Epoch: {}/{}".format(e+1, epochs))
            print("Training Loss: {:.3f}".format(running_loss/trainLoaderSize))

            ba, sen, spec, pre, f_score, test_loss, mba, emr, cdict \
                = validateModel(model, X_test, y_test, M_test)

            if ba > max_ba:
                max_ba = max(max_ba, ba)
                max_sen = max(max_sen, sen)
                max_spec = max(max_spec, spec)
                max_pre = max(max_pre, pre)
                max_f_score = max(max_f_score, f_score)
                max_test_loss = max(max_test_loss, test_loss)
                max_mba = max(max_mba, mba)
                max_emr = max(max_emr, emr)
                confusion_dict = cdict

        # lre = lre - ((0.1 - 0.01) /epochs)

    return max_ba, max_sen, max_spec, max_pre, max_f_score, max_test_loss, max_mba,\
    max_emr, cdict

def validateModel(model, X_test, y_test, M_test):

        accuracy = 0.
        running_test_loss = 0.

        tp = 0.
        fp = 0.
        tn = 0.
        fn = 0.
        exact_match = 0.

        label_wise_tp = torch.zeros(y_test.size()[1])
        label_wise_tn = torch.zeros(y_test.size()[1])
        label_wise_fp = torch.zeros(y_test.size()[1])
        label_wise_fn = torch.zeros(y_test.size()[1])

        testLoaderSize = len(X_test)/batch_size
        for i in range(0, X_test.size()[0], batch_size):

            # Turn off gradients for validation, saves memory and computations
            with torch.no_grad():

                model.eval()
                batch_x, batch_y = X_test[i:i+batch_size], y_test[i:i+batch_size]
                batch_m = M_test[i:i+batch_size]
                log_ps = model(batch_x)
                loss = criterion(log_ps, batch_y)
                running_test_loss += loss.item()

                probs = torch.sigmoid(log_ps).cpu()
                probs = np.round(probs)

                probs = np.logical_and(probs, np.logical_not(batch_m))
                # Naive accuracy (correct classification rate):
                accuracy += torch.mean((probs == batch_y).float())

                # Ground - 1 0 1 0 1
                # Actual - 0 0 1 0 1

                probs = np.logical_and(probs, np.logical_not(batch_m))
                missing_label_counts = torch.sum(batch_m)
                label_wise_missing = torch.sum(batch_m, dim=0)

                # Count occorrences of true-positive, true-negative, false-positive, and false-negative:
                tp += torch.sum(np.logical_and(probs, batch_y));
                tn += torch.sum(np.logical_and(np.logical_not(probs),np.logical_not(batch_y))) - missing_label_counts
                fp += torch.sum(np.logical_and(probs,np.logical_not(batch_y)));
                fn += torch.sum(np.logical_and(np.logical_not(probs),batch_y));

                label_wise_tp += torch.sum(np.logical_and(probs, batch_y), dim=0)
                label_wise_tn += (torch.sum(np.logical_and(np.logical_not(probs),np.logical_not(batch_y)), dim=0)  - label_wise_missing)
                label_wise_fp += torch.sum(np.logical_and(probs,np.logical_not(batch_y)), dim=0)
                label_wise_fn += torch.sum(np.logical_and(np.logical_not(probs),batch_y), dim=0)

                for j in range(len(probs)):
                    exact_match += int(torch.equal(batch_y[j].float(), probs[j].float()))

        model.train()

        confusion_dict = {'lwtp': label_wise_tp, 'lwtn': label_wise_tn, 'lwfp': label_wise_fp, \
                          'lwfn': label_wise_fn}

        label_wise_sensitivity = label_wise_tp / (label_wise_tp + label_wise_fn)
        # print("label_wise_sensitivity: ", label_wise_sensitivity)
        #
        label_wise_specificity = label_wise_tn / (label_wise_tn + label_wise_fp)
        # print("label_wise_specificity: ", label_wise_specificity)

        macro_ba = torch.mean((label_wise_sensitivity + label_wise_specificity) / 2.)
        emr = exact_match/len(X_test)

        # balanced_accuracy = torch.sum(label_wise_ba) / len(label_wise_ba)

        # Sensitivity (=recall=true positive rate) and Specificity (=true negative rate):
        sensitivity = float(tp) / (tp+fn);
        specificity = float(tn) / (tn+fp);

        # Balanced accuracy is a more fair replacement for the naive accuracy:
        balanced_accuracy = (sensitivity + specificity) / 2.;

        # Precision:
        # Beware from this metric, since it may be too sensitive to rare labels.
        # In the ExtraSensory Dataset, there is large skew among the positive and negative classes,
        # and for each label the pos/neg ratio is different.
        # This can cause undesirable and misleading results when averaging precision across different labels.
        precision = float(tp) / (tp+fp);

        f_score = (2 * precision * sensitivity) / (precision + sensitivity)

        test_loss = running_test_loss / testLoaderSize

        print("-"*10);
        print('Accuracy*:         %.2f' % (accuracy/testLoaderSize));
        print('Sensitivity (TPR): %.2f' % sensitivity);
        print('Specificity (TNR): %.2f' % specificity);
        print('Balanced accuracy: %.2f' % balanced_accuracy);
        print('Precision**:       %.2f' % precision);
        print('F1 Score:          %.2f' % f_score);
        print('Macro BA:          %.2f' % macro_ba);
        print('EMR:               %.2f' % emr);

        test_losses.append(test_loss)
        accuracies.append(balanced_accuracy)

        print("Test Loss: {:.3f} ".format(test_loss))
        print("-"*10);

        #      "Test Accuracy: {:.3f}".format(accuracy/testLoaderSize))

        return balanced_accuracy, sensitivity, specificity, precision, f_score,\
                    test_loss, macro_ba, emr, confusion_dict

def plot_loss(train_loss, test_loss, title, x_axis, y_axis):

    plt.plot(train_loss, '-b', label='train_loss')
    plt.plot(test_loss, '-r', label='test_loss')

    plt.xlabel(x_axis)
    plt.ylabel(y_axis)

    plt.legend(loc='upper right')
    plt.title(title)

    # plt.show()

def plot_balanced_accuracy(accuracy_values, title = 'Balanced Accuracy'):

    plt.xlabel('iterations')
    plt.ylabel('BA')
    plt.title(title)
    plt.plot(accuracy_values)
    # plt.show()

def plot_lwtp(tp_values, iter, sample_strategy, title = 'Label wise TP'):

    plt.xlabel('Labels')
    plt.ylabel('lwtp')
    plt.title(title)

    for i in range(0, (len(label_names)), 15):
        plt.bar(label_names[i:i+15], tp_values[i:i+15])
        plt.savefig('lwtp_' + sample_strategy + '_' + str(iter) + '_' + str(i) + '.png')
        plt.clf()
    # plt.show()

def plot_labelled_metrics(labelled_examples, accuracy_values, sensitivity_values,
                     specificity_values, precision_values, f_score_values, \
                      plot_label, title = 'Metrics',):

    plt.plot(labelled_examples, accuracy_values, label='Accuracy')
    plt.plot(labelled_examples, sensitivity_values, label='Sensitivity')
    plt.plot(labelled_examples, specificity_values, label='Specificity')
    plt.plot(labelled_examples, precision_values, label='Precision')
    plt.plot(labelled_examples, f_score_values, label='F-Score')
    plt.xlabel('labelled_examples')
    plt.ylabel('Metrics')
    plt.legend(loc="upper right")
    plt.title(title)

def plot_labelled_ba(labelled_examples, accuracy_values,
                      plot_label, title = 'Balanced Accuracy',):

    plt.plot(labelled_examples, accuracy_values, label=plot_label)
    plt.xlabel('labelled_examples')
    plt.ylabel('Metrics')
    plt.legend(loc="upper right")
    plt.title(title)

def estimate_standardization_params(X_train):
    mean_vec = np.nanmean(X_train,axis=0);
    std_vec = np.nanstd(X_train,axis=0);
    return (mean_vec,std_vec);

def standardize_features(X,mean_vec,std_vec):
    # Subtract the mean, to centralize all features around zero:
    X_centralized = X - mean_vec.reshape((1,-1));
    # Divide by the standard deviation, to get unit-variance for all features:
    # * Avoid dividing by zero, in case some feature had estimate of zero variance
    normalizers = np.where(std_vec > 0., std_vec, 1.).reshape((1,-1));
    X_standard = X_centralized / normalizers;
    return X_standard;

def sample_unlabelled_data(model, X_unlabelled, sample_strategy='random'):

    sample_indices = 0
    confidence_list = []

    if sample_strategy == 'random':

        sample_indices = torch.randperm(X_unlabelled.size()[0])
        # model.eval()

        # # Turn off gradients for validation, saves memory and computations
        # with torch.no_grad():

        for i in sample_indices:

            # item = X_unlabelled[i]

            # # batch_x = X_unlabelled[i:i+batch_size]
            # log_ps = model(item)
            # preds = torch.sigmoid(log_ps)
            # preds = np.round(preds)

            conf = []
            conf.append(i)
            # conf.append(preds)
            confidence_list.append(conf)

    elif sample_strategy == 'lcp':

        confidence_list = []
        model.eval()

        # Turn off gradients for validation, saves memory and computations
        with torch.no_grad():

            for i, item in enumerate(X_unlabelled):

                # batch_x = X_unlabelled[i:i+batch_size]
                log_ps = model(item)
                # probs = torch.exp(log_ps).cpu()
                pos_probs = torch.sigmoid(log_ps).cpu()
                neg_probs = 1 - pos_probs

                lcp = torch.min(pos_probs, neg_probs)
                # lcp = 1 - lcp
                lcp = (torch.sum(lcp * 2)) / lcp.numel()

                conf = []
                conf.append(i)
                conf.append(lcp)
                confidence_list.append(conf)

        confidence_list.sort(key=lambda x: x[1], reverse=True)
        sample_indices = torch.tensor([item[0] for item in confidence_list], \
                                      dtype = torch.long)

    elif sample_strategy == 'max_lcp':

        confidence_list = []
        model.eval()

        # Turn off gradients for validation, saves memory and computations
        with torch.no_grad():

            for i, item in enumerate(X_unlabelled):

                log_ps = model(item)
                pos_probs = torch.sigmoid(log_ps).cpu()
                neg_probs = 1 - pos_probs

                lcp = torch.min(pos_probs, neg_probs)
                # lcp = 1 - lcp
                max_lcp = torch.max(lcp)

                conf = []
                conf.append(i)
                conf.append(max_lcp)
                confidence_list.append(conf)

        confidence_list.sort(key=lambda x: x[1], reverse=True)
        sample_indices = torch.tensor([item[0] for item in confidence_list], \
                                      dtype = torch.long)

    elif sample_strategy == 'entropy':

        model.eval()

        # Turn off gradients for validation, saves memory and computations
        with torch.no_grad():

            for i, item in enumerate(X_unlabelled):

                # batch_x = X_unlabelled[i:i+batch_size]
                log_ps = model(item)
                # probs = torch.exp(log_ps).cpu()
                # m = torch.nn.Softmax(dim = 0)
                # probs = m(log_ps)
                pos_probs = torch.sigmoid(log_ps).cpu()
                neg_probs = 1 - pos_probs

                # log_probs = probs * torch.log2(probs)
                pos_log_probs = pos_probs * torch.log2(pos_probs)
                neg_log_probs = neg_probs * torch.log2(neg_probs)
                log_probs = pos_log_probs + neg_log_probs
                raw_entropy = 0 - torch.sum(log_probs)

                normalized_entropy = raw_entropy / math.log(log_probs.numel(), 2)

                preds = torch.sigmoid(log_ps).cpu()
                preds = np.round(preds)

                conf = []
                conf.append(i)
                conf.append(preds)
                conf.append(normalized_entropy.item())
                confidence_list.append(conf)

        confidence_list.sort(key=lambda x: x[2], reverse=True)
        sample_indices = torch.tensor([item[0] for item in confidence_list], \
                                      dtype = torch.long)

    elif sample_strategy == 'max_entropy':

        model.eval()

        # Turn off gradients for validation, saves memory and computations
        with torch.no_grad():

            for i, item in enumerate(X_unlabelled):

                log_ps = model(item)
                pos_probs = torch.sigmoid(log_ps).cpu()
                neg_probs = 1 - pos_probs

                # log_probs = probs * torch.log2(probs)
                pos_log_probs = pos_probs * torch.log2(pos_probs)
                neg_log_probs = neg_probs * torch.log2(neg_probs)
                log_probs = -(pos_log_probs + neg_log_probs)
                max_entropy = torch.max(log_probs)

                preds = torch.sigmoid(log_ps).cpu()
                preds = np.round(preds)

                conf = []
                conf.append(i)
                conf.append(preds)
                conf.append(max_entropy.item())
                confidence_list.append(conf)

        confidence_list.sort(key=lambda x: x[2], reverse=True)
        sample_indices = torch.tensor([item[0] for item in confidence_list], \
                                      dtype = torch.long)

    elif sample_strategy == 'mean_margin':

        model.eval()

        # Turn off gradients for validation, saves memory and computations
        with torch.no_grad():

            for i, item in enumerate(X_unlabelled):

                # batch_x = X_unlabelled[i:i+batch_size]
                log_ps = model(item)
                # probs = torch.exp(log_ps).cpu()
                # m = torch.nn.Softmax(dim = 0)
                # probs = m(log_ps)
                pos_probs = torch.sigmoid(log_ps).cpu()
                neg_probs = 1. - pos_probs
                margin = abs(pos_probs - neg_probs)
                mean_margin = torch.mean(margin)

                preds = np.round(pos_probs)

                conf = []
                conf.append(i)
                conf.append(preds)
                conf.append(mean_margin.item())
                confidence_list.append(conf)

        confidence_list.sort(key=lambda x: x[2])
        sample_indices = torch.tensor([item[0] for item in confidence_list], \
                                      dtype = torch.long)

    elif sample_strategy == 'min_margin':

        model.eval()

        # Turn off gradients for validation, saves memory and computations
        with torch.no_grad():

            for i, item in enumerate(X_unlabelled):

                log_ps = model(item)
                pos_probs = torch.sigmoid(log_ps).cpu()
                neg_probs = 1. - pos_probs
                margin = abs(pos_probs - neg_probs)
                min_margin = torch.min(margin)

                preds = np.round(pos_probs)

                conf = []
                conf.append(i)
                conf.append(preds)
                conf.append(min_margin.item())
                confidence_list.append(conf)

        confidence_list.sort(key=lambda x: x[2])
        sample_indices = torch.tensor([item[0] for item in confidence_list], \
                                      dtype = torch.long)

    model.train()
    return confidence_list

# Strategies can be random or classwise
def test_train_split(X, Y, M, percentage = 0.7, strategy = 'random'):

    if strategy == 'classwise':
        pass
    else:
        sample_indices = np.random.permutation(len(X)).tolist()
        test_length = (int)((1-percentage) * len(X))

        X_test = torch.tensor(X.iloc[sample_indices[:test_length]].values, \
                    dtype=torch.float32)
        (mean_vec,std_vec) = estimate_standardization_params(X_test);
        X_test = standardize_features(X_test,mean_vec,std_vec);
        y_test = torch.tensor(Y.iloc[sample_indices[:test_length]].values, \
                    dtype=torch.float32)

        M_test = torch.tensor(M.iloc[sample_indices[:test_length]].values, \
                    dtype=torch.float32)

        X.reset_index(inplace=True)
        Y.reset_index(inplace=True)
        M.reset_index(inplace=True)

        X.drop(X.index[sample_indices[:test_length]], inplace = True)
        Y.drop(Y.index[sample_indices[:test_length]], inplace = True)
        M.drop(M.index[sample_indices[:test_length]], inplace = True)

        del X['index']
        del Y['index']
        del M['index']

        X_train = torch.tensor(X.values, dtype=torch.float32)
        (mean_vec,std_vec) = estimate_standardization_params(X_train);
        X_train = standardize_features(X_train,mean_vec,std_vec);
        y_train = torch.tensor(Y.values, dtype=torch.float32)

        M_train = torch.tensor(M.values.astype(np.float32))

        return X_train, y_train, X_test, y_test, M_train, M_test

def train_without_AL(X_train, y_train, X_test, y_test, M_train, M_test, model):

    plt.clf()

    instance_weights_train = get_class_weights(y_train, M_train)

    # Train and Evaluate Model
    ba, sen, spec, pre, f_score, test_loss, mba, emr, cdict =  \
        trainAndTestModel(model, X_train.float(), y_train, X_test.float(), y_test, \
                      instance_weights_train, M_test)

    plot_loss(train_losses, test_losses, 'Loss', 'iterations', 'loss')
    plt.savefig('Loss_warm_seed.png')
    plt.clf()

    plot_balanced_accuracy(accuracies, 'BA without AL')
    plt.savefig('BA_warm_seed.png')
    plt.clf()

    return ba, sen, spec, pre, f_score, test_loss, mba, emr, cdict


def data_description(Y, M):
    plt.clf()
    plt.rcParams.update({'font.size': 13})

    Y = Y.numpy()
    M = M.numpy()
    positive_Y = [item for item in np.sum(Y, axis=0)]
    missing_Y = [item for item in np.sum(M, axis=0)]
    total = [len(Y) - i for i in missing_Y]
    total = [total[i] - positive_Y[i] for i in range(len(total))]

    df=pd.DataFrame({'Label Names': label_names, 'Positive':positive_Y, 'Negative':total})
    df.set_index(["Label Names"],inplace=True)
    df.plot(kind='bar', figsize=(15,15), stacked=True)

    labels_and_counts = zip(label_names, positive_Y)

def stratify_samples(y_unlabelled_AL, sample_indices, threshold):

    stratified_sample_indicies = set()
    num_of_labels = len(y_unlabelled_AL[0])

    for label in range(num_of_labels):
        examples = 0
        for i, y in enumerate(y_unlabelled_AL):
            if examples < threshold and y[label] == 1:
                stratified_sample_indicies.add(sample_indices[i])
                examples += 1

    rest_of_indices = []

    for i in sample_indices:

        if i in stratified_sample_indicies:
            continue
        else:
            rest_of_indices.append(i)

    return list(stratified_sample_indicies), rest_of_indices

def train_AL(X_initial, y_initial, M_initial, X_unlabelled, y_unlabelled, X_test, y_test, M_unlabelled, M_test ,\
             model, sample_strategy = 'random', threshold = 20, sample_size=50, diversity = False):


    X_train =  X_initial
    y_train =  y_initial
    M_train =  M_initial

    total_size = len(X_unlabelled)

    iter = 1

    AL_accuracies, AL_test_loss, labelled_examples, AL_emr = [], [], [], []
    AL_Sen, AL_Spec, AL_precision, AL_f_score, AL_mba = [], [], [], [], []
    AL_confusion_dict = []

    ba, sen, spec, pre, f_score, test_loss, mba, emr, cdict \
            = validateModel(model, X_test, y_test, M_test)

    labelled_examples.append(X_train.size()[0])
    AL_accuracies.append(ba)
    AL_Sen.append(sen)
    AL_Spec.append(spec)
    AL_precision.append(pre)
    AL_f_score.append(f_score)
    AL_test_loss.append(test_loss)
    AL_mba.append(mba)
    AL_emr.append(emr)
    AL_confusion_dict.append(cdict)

    labelled_size = 0
    # unlabelled_size = 0
    default_sample_size = sample_size

    while X_train.size()[0] < total_size:
    # while labelled_size < 1000:

        print("Active Learning Iteration: ", iter, "Sampling strategy: ", sample_strategy)

        conf_list = sample_unlabelled_data(model, X_unlabelled.float(), sample_strategy)
        sample_indices = torch.tensor([item[0] for item in conf_list], dtype=torch.long)
        # y_preds = [item[1] for item in conf_list]

        # strat_sample_indices, rest_of_indices = stratify_samples(y_preds, sample_indices, threshold)
        # # print(torch.sum(y_unlabelled_AL[sample_indices[:sample_size]],dim=0))
        # sample_indices = strat_sample_indices + rest_of_indices

        # if len(strat_sample_indices) > default_sample_size:
        #     sample_size = len(strat_sample_indices)
        # else:
        #     sample_size = default_sample_size

        X_train = torch.cat((X_train, X_unlabelled[sample_indices[:sample_size]]), 0)
        y_train = torch.cat((y_train, y_unlabelled[sample_indices[:sample_size]]), 0)
        M_train = torch.cat((M_train, M_unlabelled[sample_indices[:sample_size]]), 0)

        labelled_size += sample_size

        # print(len(X_train))

        # describe_data(y_train, iter)
        # plt.savefig(sample_strategy + 'label_distribution.png')
        # plt.clf()

        X_unlabelled = X_unlabelled[sample_indices[sample_size:]]
        y_unlabelled = y_unlabelled[sample_indices[sample_size:]]
        M_unlabelled = M_unlabelled[sample_indices[sample_size:]]

        instance_weights_train = get_class_weights(y_train, M_train)

        # Train and Evaluate Model
        ba, sen, spec, pre, f_score, test_loss, mba, emr, cdict = trainAndTestModel(model, \
                                        X_train.float(), y_train, \
                                          X_test.float(), y_test, \
                      instance_weights_train, M_test)

        # plot_lwtp(lwtp, iter, sample_strategy)

        AL_accuracies.append(ba)
        AL_Sen.append(sen)
        AL_Spec.append(spec)
        AL_precision.append(pre)
        AL_f_score.append(f_score)
        AL_test_loss.append(test_loss)
        AL_mba.append(mba)
        AL_emr.append(emr)
        labelled_examples.append(X_train.size()[0])
        AL_confusion_dict.append(cdict)

        print("Total number of labelled samples %d" % (X_train.size()[0]))

        iter = iter + 1

        if iter % 1 == 0:
            metrics_dict = {'labelled_examples':labelled_examples, 'ba':AL_accuracies,\
                       'sen': AL_Sen, 'spec': AL_Spec, 'f_score': AL_f_score, \
                       'precision': AL_precision, \
                       'mba': AL_mba, 'emr': AL_emr, 'cdict': AL_confusion_dict}

            write_to_pickle(metrics_dict, 'metrics_' + sample_strategy + '.pickle')

    # plt.savefig(sample_strategy + 'label_distribution.png')
    # plt.clf()

    return labelled_examples, AL_accuracies, AL_Sen, AL_Spec, AL_precision, \
    AL_f_score, AL_mba, AL_emr

def create_plotLine(ba_full, start_size, total_size, sample_size):

    full_acc = []
    labelled_examples = []
    for i in range(start_size, total_size+1, sample_size):
        labelled_examples.append(i)
        full_acc.append(ba_full)
    return labelled_examples, full_acc

def write_to_pickle(obj, filename):
    with open(filename, 'wb') as f:
        pickle.dump(obj, f)

# Specify a path
PATH = "initial_model_dict.pt"

def full_supervised_training():
    users, X, Y, M, timestamps, label_names = \
            read_data_from_all_users(source_dir='../../ESDataset/')

    # Prepare training and test data
    # M = np.isnan(Y)
    # M = M.iloc[:, np.r_[0:42]]

    # Also, there may be missing sensor-features (represented in the data as NaN).
    # You can handle those by imputing a value of zero (since we standardized, this is equivalent to assuming average value).
    # You can also further select examples - only those that have values for all the features.
    # For this tutorial, let's use the simple heuristic of zero-imputation:
    X = X.iloc[:, np.r_[0:52, 83:181, 183:209]]
    # M = M.iloc[:, np.r_[1:3,4:5,10:12,16:17,26:27,28:29,31:32,43:44]]
    # Y = Y.iloc[:, np.r_[1:3,4:5,10:12,16:17,26:27,28:29,31:32,43:44]]
    X[np.isnan(X)] = 0.
    Y[np.isnan(Y)] = 0.

    X_train, y_train, X_test, y_test, M_train, M_test = test_train_split(X, Y, M)

    model_full_supervised = Network()
    ba_full, sen_full, spec_full, pre_full, f_score_full, test_loss_full, mba_full, emr_full, cdict_full = \
    train_without_AL(X_train, y_train, X_test, y_test, M_train, M_test, model_full_supervised)

    metrics_full_dict = {'ba_full':ba_full,\
               'sen_full': sen_full, 'spec_full': spec_full, 'pre_full': pre_full, \
               'f_score_full': f_score_full,\
               'mba_full': mba_full, 'emr_full': emr_full, 'cdict_full': cdict_full}

    write_to_pickle(metrics_full_dict, 'metrics_full.pickle')

    model_initial = Network()

    random_indices = np.random.permutation(len(X_test)).tolist()
    warm_seed_train_length = (int)(0.07 * len(X_test))

    X_initial = X_test[random_indices[:warm_seed_train_length]]
    y_initial = y_test[random_indices[:warm_seed_train_length]]
    X_test_rand = X_test[random_indices[warm_seed_train_length:]]
    y_test_rand = y_test[random_indices[warm_seed_train_length:]]
    M_initial = M_test[random_indices[:warm_seed_train_length]]
    M_test_rand = M_test[random_indices[warm_seed_train_length:]]

    print("Split done")
    plt.clf()
    train_without_AL(X_initial, y_initial, X_test_rand, y_test_rand, M_initial, M_test_rand, model_initial)

    # Save initial model
    torch.save(model_initial.state_dict(), PATH)

    print("Warm seed training done")

    X_train_rand = torch.cat((X_train, X_test_rand), 0)
    y_train_rand = torch.cat((y_train, y_test_rand), 0)
    M_train_rand = torch.cat((M_train, M_test_rand), 0)

    random_indices = np.random.permutation(len(X_train_rand)).tolist()
    train_length = (int)(0.7 * len(X_train_rand))

    X_unlabelled_rand = X_train_rand[random_indices[:train_length]]
    y_unlabelled_rand = y_train_rand[random_indices[:train_length]]
    X_test_rand = X_train_rand[random_indices[train_length:]]
    y_test_rand = y_train_rand[random_indices[train_length:]]
    M_unlabelled_rand = M_train_rand[random_indices[:train_length]]
    M_test_rand = M_train_rand[random_indices[train_length:]]


    split_data_dict = {'X_unlabelled_rand':X_unlabelled_rand, 'y_unlabelled_rand':y_unlabelled_rand,\
                           'M_unlabelled_rand': M_unlabelled_rand, 'X_test_rand': X_test_rand, 'y_test_rand': y_test_rand,\
                           'M_test_rand': M_test_rand, 'X_initial': X_initial, 'y_initial': y_initial, 'M_initial': M_initial}

    write_to_pickle(split_data_dict, 'data_split.pickle')

def main():

    sampling_strategy = sys.argv[1]

    if sampling_strategy == 'full':
        full_supervised_training()

    else:
        ssize = int(sys.argv[2])
        split_data_dict = pd.read_pickle(r'data_split.pickle')

        X_unlabelled_rand = split_data_dict['X_unlabelled_rand']
        y_unlabelled_rand = split_data_dict['y_unlabelled_rand']
        M_unlabelled_rand = split_data_dict['M_unlabelled_rand']
        X_test_rand = split_data_dict['X_test_rand']
        y_test_rand = split_data_dict['y_test_rand']
        M_test_rand = split_data_dict['M_test_rand']
        X_initial = split_data_dict['X_initial']
        y_initial = split_data_dict['y_initial']
        M_initial = split_data_dict['M_initial']

        model_initial = Network()
        model_initial.load_state_dict(torch.load(PATH))

        model_strategy = copy.deepcopy(model_initial)

        labelled_examples, ba, sen, spec, precision, \
                       f_score, mba, emr = train_AL(X_initial, y_initial, M_initial, \
                                               X_unlabelled_rand, y_unlabelled_rand, X_test_rand, \
                                               y_test_rand, M_unlabelled_rand, M_test_rand,
                                               model = model_strategy, \
                                               sample_strategy = sampling_strategy, \
                                               sample_size=ssize)

if __name__ == "__main__":
    main()

# model_random = copy.deepcopy(model_initial)

# labelled_examples_random, ba_random, sen_random, spec_random, precision_random, \
#                f_score_random, mba_random, emr_random = train_AL(X_initial, y_initial, M_initial, \
#                                        X_unlabelled_rand, y_unlabelled_rand, X_test_rand, \
#                                        y_test_rand, M_unlabelled_rand, M_test_rand,
#                                        model = model_random, \
#                                        sample_strategy = 'random', \
#                                        sample_size=1000)

# plot_labelled_metrics(labelled_examples_random, ba_random, sen_random, spec_random, \
#                      precision_random, f_score_random, plot_label='Random')

# metrics_random_dict = {'labelled_examples_random':labelled_examples_random, 'ba_random':ba_random,\
#                        'sen_random': sen_random, 'spec_random': spec_random, 'f_score_random': f_score_random,\
#                        'mba_random': mba_random, 'emr_random': emr_random}

# write_to_pickle(metrics_random_dict, 'metrics_random.pickle')

# plt.savefig('AL_validate_unlabelled_random.png')
# plt.clf()

# model_entropy = copy.deepcopy(model_initial)

# labelled_examples_entropy, ba_entropy, sen_entropy, spec_entropy, precision_entropy, \
#    f_score_entropy, mba_entropy, emr_entropy = train_AL(X_initial, y_initial, M_initial, \
#                               X_unlabelled_rand, y_unlabelled_rand, X_test_rand, \
#                                        y_test_rand, M_unlabelled_rand, M_test_rand,
#                                        model = model_entropy, \
#                                        sample_strategy = 'entropy', \
#                                        sample_size=1000)

# plot_labelled_metrics(labelled_examples_entropy, ba_entropy, sen_entropy, spec_entropy, \
#                      precision_entropy, f_score_entropy, plot_label='Entropy')

# metrics_entropy_dict = {'labelled_examples_entropy':labelled_examples_entropy, 'ba_entropy':ba_entropy,\
#                       'sen_entropy': sen_entropy, 'spec_entropy': spec_entropy, 'f_score_entropy': f_score_entropy,\
#                       'mba_entropy': mba_entropy, 'emr_random': emr_random}

# write_to_pickle(metrics_entropy_dict, 'metrics_entropy.pickle')

# plt.savefig('AL_validate_unlabelled_entropy.png')
# plt.clf()

# model_mean_margin = copy.deepcopy(model_initial)

# labelled_examples_mm, ba_mm, sen_mm, spec_mm, precision_mm, \
#    f_score_mm, mba_mm, emr_mm = train_AL(X_initial, y_initial, M_initial, \
#                                        X_unlabelled_rand, y_unlabelled_rand, X_test_rand, \
#                                        y_test_rand, M_unlabelled_rand, M_test_rand, \
#                                        model = model_mean_margin, \
#                                        sample_strategy = 'mean_margin', \
#                                        sample_size=1000)

# plot_labelled_metrics(labelled_examples_mm, ba_mm, sen_mm, spec_mm, precision_mm, \
#    f_score_mm, plot_label='Mean Margin')

# metrics_mm_dict = {'labelled_examples_mm':labelled_examples_mm, 'ba_mm':ba_mm,\
#                        'sen_mm': sen_mm, 'spec_mm': spec_mm, 'f_score_mm': f_score_mm,\
#                        'mba_mm': mba_mm, 'emr_mm': emr_mm}

# write_to_pickle(metrics_mm_dict, 'metrics_mm.pickle')

# plt.savefig('AL_validate_unlabelled_mean_margin.png')
# plt.clf()


# model_lcp = copy.deepcopy(model_initial)
#
# labelled_examples_lcp, ba_lcp, sen_lcp, spec_lcp, precision_lcp, \
#    f_score_lcp, mba_lcp, emr_lcp = train_AL(X_initial, y_initial, M_initial, \
#                                        X_unlabelled_rand, y_unlabelled_rand, X_test_rand, \
#                                        y_test_rand, M_unlabelled_rand, M_test_rand, \
#                                        model = model_lcp, \
#                                        sample_strategy = 'lcp', \
#                                        sample_size=1000)
#
# plot_labelled_metrics(labelled_examples_lcp, ba_lcp, sen_lcp, spec_lcp, precision_lcp, \
#    f_score_lcp, plot_label='LCP')
#
# metrics_lcp_dict = {'labelled_examples_lcp':labelled_examples_lcp, 'ba_lcp':ba_lcp,\
#                        'sen_lcp': sen_lcp, 'spec_lcp': spec_lcp, 'f_score_lcp': f_score_lcp,\
#                        'mba_lcp': mba_lcp, 'emr_lcp': emr_lcp}
#
# write_to_pickle(metrics_lcp_dict, 'metrics_lcp.pickle')
#
# plt.savefig('AL_validate_unlabelled_lcp.png')
# plt.clf()
